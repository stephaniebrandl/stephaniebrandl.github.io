@article{brandl2022every,
  title={Every word counts: A multilingual analysis of individual human alignment with model attention},
  author={Brandl, S. and Hollenstein N.},
  journal={arXiv e-prints},
  pages={arXiv--2210},
  pdf={https://arxiv.org/pdf/2210.04963.pdf},
  abbr={AACL},
  selected={true},
  year={2022},
  code={https://github.com/stephaniebrandl/eyetracking-subgroups},
  tldr={https://twitter.com/StephanieBrandl/status/1574712904175259648?s=20&t=rSziizmyafl5IxqjOgsOeA},
  abstract={Human fixation patterns have been shown to correlate strongly with Transformer-based attention. Those correlation analyses are usually carried out without taking into account individual differences between participants and are mostly done on monolingual datasets making it difficult to generalise findings. In this paper, we analyse eye-tracking data from speakers of 13 different languages reading both in their native language (L1) and in English as language learners (L2). We find considerable differences between languages but also that individual reading behaviour such as skipping rate, total reading time and vocabulary knowledge (LexTALE) influence the alignment between humans and models to an extent that should be considered in future studies.},
}

@article{brandl2022domain,
  title={Domain-Specific Word Embeddings with Structure Prediction},
  author={Brandl, S.* and Lassner, D.* and Baillot A. and Nakajima S.},
  journal={arXiv e-prints},
  pages={arXiv--2210},
  pdf={https://arxiv.org/pdf/2210.04962.pdf},
  abbr={TACL},
  selected={true},
  year={2022},
  tldr={https://twitter.com/StephanieBrandl/status/1580128968673734661?s=20&t=dcHLxlBouKiUgVKE7FvlZw},
  abstract={Complementary to finding good general word embeddings, an important question for representation learning is to find dynamic word embeddings, e.g., across time or domain. Current methods do not offer a way to use or predict information on structure between sub-corpora, time or domain and dynamic embeddings can only be compared after post-alignment. We propose novel word embedding methods that provide general word representations for the whole corpus, domain-specific representations for each sub-corpus, sub-corpus structure, and embedding alignment simultaneously. We present an empirical evaluation on New York Times articles and two English Wikipedia datasets with articles on science and philosophy. Our method, called Word2Vec with Structure Prediction (W2VPred), provides better performance than baselines in terms of the general analogy tests, domain-specific analogy tests, and multiple specific word embedding evaluations as well as structure prediction performance when no structure is given a priori. As a use case in the field of Digital Humanities we demonstrate how to raise novel research questions for high literature from the German Text Archive.},
}

@inproceedings{morger-etal-2022-cross,
    title = "A Cross-lingual Comparison of Human and Model Relative Word Importance",
    author = "Morger, F.  and
      Brandl, S.  and
      Beinborn, L.  and
      Hollenstein, N.",
    booktitle = "Proceedings of the 2022 CLASP Conference on (Dis)embodiment",
    month = sep,
    year = "2022",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.clasp-1.2",
    pages = "11--23",
    code = https://github.com/felixhultin/cross_lingual_relative_importance,
    abstract = "Relative word importance is a key metric for natural language processing. In this work, we compare human and model relative word importance to investigate if pretrained neural language models focus on the same words as humans cross-lingually. We perform an extensive study using several importance metrics (gradient-based saliency and attention-based) in monolingual and multilingual models, including eye-tracking corpora from four languages (German, Dutch, English, and Russian). We find that gradient-based saliency, first-layer attention, and attention flow correlate strongly with human eye-tracking data across all four languages. We further analyze the role of word length and word frequency in determining relative importance and find that it strongly correlates with length and frequency, however, the mechanisms behind these non-linear relations remain elusive. We obtain a cross-lingual approximation of the similarity between human and computational language processing and insights into the usability of several importance metrics.",
}


@inproceedings{brandl2022evaluating,
  title={Evaluating Deep Taylor Decomposition for Reliability Assessment in the Wild},
  author={Brandl, S. and Hershcovich, D. and S{\o}gaard, A.},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={16},
  pages={1368--1372},
  year={2022},
  tldr={https://twitter.com/StephanieBrandl/status/1534130582933880837?s=20&t=rSziizmyafl5IxqjOgsOeA},
  code={https://github.com/coastalcph/reliability-wild},
}

@inproceedings{eberle2022transformer,
  title={Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
  author={Eberle, O.* and Brandl, S.* and Pilot, J. and S{\o}gaard, A.},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4295--4309},
  year={2022},
  code={https://github.com/oeberle/task_gaze_transformers},
  abbr={ACL},
  selected={true},
  abstract={Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts.Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.},
  pdf={https://aclanthology.org/2022.acl-long.296.pdf},
}

@inproceedings{brandl-etal-2022-conservative,
    title = "How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns",
    author = "Brandl, S.  and
      Cui, R.  and
      S{\o}gaard, A.",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2022.naacl-main.265",
    doi = "10.18653/v1/2022.naacl-main.265",
    pages = "3624--3630",
    abstract = "Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.",
  selected={true},
  code={https://github.com/stephaniebrandl/gender-neutral-pronouns},
  tldr={https://twitter.com/StephanieBrandl/status/1519352654899691520?s=20&t=rSziizmyafl5IxqjOgsOeA},
  abbr={NAACL},
}

@inproceedings{hershcovich2022challenges,
  title={Challenges and Strategies in Cross-Cultural NLP},
  author={Hershcovich, D. and Frank, S. and Lent, H. and de Lhoneux, M. and Abdou, M. and Brandl, S. and Bugliarello, E. and Cabello Piqueras, L. and Chalkidis, I. and Cui, R. and Fierro, C. and Margatina, K. and Rust, P. and Søgaard, A.},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6997--7013},
  year={2022},
  abbr={ACL},
  selected={true},
  abstract={Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.},
  pdf={https://aclanthology.org/2022.acl-long.482.pdf},
  tldr={https://twitter.com/daniel_hers/status/1505829084210868224?s=20&t=rSziizmyafl5IxqjOgsOeA},
}

@article{brandl2016brain,
  title={Brain--Computer Interfacing under Distraction: An Evaluation Study},
  author={Brandl, Stephanie and Fr{\o}lich, Laura and H{\"o}hne, Johannes and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={Journal of Neural Engineering},
  volume={13},
  number={5},
  pages={056012},
  year={2016},
  publisher={IOP Publishing}
}

@inproceedings{brandl2015bringing,
  title={Bringing BCI into everyday life: Motor imagery in a pseudo realistic environment},
  author={Brandl, Stephanie and H{\"o}hne, Johannes and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  booktitle={2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)},
  pages={224--227},
  year={2015},
  organization={IEEE}
}

@inproceedings{brandl2015robust,
  title={Robust common spatial patterns based on Bhattacharyya distance and Gamma divergence},
  author={Brandl, Stephanie and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  booktitle={The 3rd International Winter Conference on Brain-Computer Interface},
  pages={1--4},
  year={2015},
  organization={IEEE}
}

@misc{brandl2015distraction,
   author = {Brandl, Stephanie},
   title = {BCI under distraction: Motor imagery in a pseudo realistic environment},
   year = {2015},
   doi = {10.14279/depositonce-9827.2},
   note = {http://dx.doi.org/10.14279/depositonce-9827.2}
}

@inproceedings{brandl2016alternative,
  title={Alternative CSP approaches for multimodal distributed BCI data},
  author={Brandl, Stephanie and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  booktitle={2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={003742--003747},
  year={2016},
  organization={IEEE}
}

@article{brandl2020motor,
  title={Motor Imagery Under Distraction—An Open Access BCI Dataset},
  author={Brandl, Stephanie and Blankertz, Benjamin},
  journal={Frontiers in neuroscience},
  volume={14},
  pages={967},
  year={2020},
  publisher={Frontiers}
}

@article {brandl2021fourier,
	author = {Brandl, Stephanie and Haumann, Niels Trusbak and Radloff, Simjon and D{\"a}hne, Sven and Bonetti, Leonardo and Vuust, Peter and Brattico, Elvira and Grube, Manon},
	title = {Fourier SPoC: A customised machine-learning analysis pipeline for auditory beat-based entrainment in the MEG\makebox[0pt][l]{}\phantom{}},
	elocation-id = {2021.04.23.441088},
	year = {2021},
	doi = {10.1101/2021.04.23.441088},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2021/04/23/2021.04.23.441088},
	eprint = {https://www.biorxiv.org/content/early/2021/04/23/2021.04.23.441088.full.pdf},
	journal = {BioRxiv}
}

@inproceedings{brandl2019times,
  title={Times Are Changing: Investigating the Pace of Language Change in Diachronic Word Embeddings},
  author={Brandl, Stephanie and Lassner, David},
  booktitle={Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change},
  pages={146--150},
  year={2019}
}

@article{brandl2020balancing,
  title={Balancing the composition of word embeddings across heterogenous data sets},
  author={Brandl, Stephanie and Lassner, David and Alber, Maximilian},
  journal={arXiv preprint arXiv:2001.04693},
  year={2020}
}

@article{brandl2020early,
  title={Early Corona Twitter Dataset},
  author={Brandl, Stephanie and Lassner, David},
journal={HAL preprint hal-02861167},  
  year={2020}
 
}

@misc {brandl2020corona,
   author = {Brandl, Stephanie AND Lassner, David},
   title = {Corona Twitter Dataset: 16 February 2020 - 03 March 2020},
   year = {2020},
   doi = {10.14279/depositonce-10012},
   note = {http://dx.doi.org/10.14279/depositonce-10012}
}


